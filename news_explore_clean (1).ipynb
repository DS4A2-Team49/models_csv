{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4c4e442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting matplotlib\n",
      "  Using cached matplotlib-3.4.2-cp38-cp38-manylinux1_x86_64.whl (10.3 MB)\n",
      "Requirement already satisfied: numpy>=1.16 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (1.20.3)\n",
      "Collecting pillow>=6.2.0\n",
      "  Using cached Pillow-8.3.1-cp38-cp38-manylinux_2_5_x86_64.manylinux1_x86_64.whl (3.0 MB)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Using cached kiwisolver-1.3.1-cp38-cp38-manylinux1_x86_64.whl (1.2 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Using cached cycler-0.10.0-py2.py3-none-any.whl (6.5 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (2.8.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib) (2.4.7)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
      "Installing collected packages: pillow, kiwisolver, cycler, matplotlib\n",
      "Successfully installed cycler-0.10.0 kiwisolver-1.3.1 matplotlib-3.4.2 pillow-8.3.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bbc2d607",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.6.2-py3-none-any.whl (1.5 MB)\n",
      "Collecting joblib\n",
      "  Using cached joblib-1.0.1-py3-none-any.whl (303 kB)\n",
      "Requirement already satisfied: tqdm in /opt/conda/lib/python3.8/site-packages (from nltk) (4.60.0)\n",
      "Collecting regex\n",
      "  Using cached regex-2021.7.6-cp38-cp38-manylinux2014_x86_64.whl (737 kB)\n",
      "Requirement already satisfied: click in /opt/conda/lib/python3.8/site-packages (from nltk) (8.0.1)\n",
      "Installing collected packages: regex, joblib, nltk\n",
      "Successfully installed joblib-1.0.1 nltk-3.6.2 regex-2021.7.6\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01c4defe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/jovyan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer \n",
    "import re\n",
    "from datetime import date, datetime, timedelta\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "3cf81747",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Unnamed: 0.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1.1</th>\n",
       "      <th>Unnamed: 0.1.1.1.1.1.1</th>\n",
       "      <th>index</th>\n",
       "      <th>title</th>\n",
       "      <th>media</th>\n",
       "      <th>date</th>\n",
       "      <th>datetime</th>\n",
       "      <th>desc</th>\n",
       "      <th>link</th>\n",
       "      <th>img</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>Seattle police release video of fatal shooting...</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>01/01/2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Faletogo died of a gunshot wound to the head, ...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/crim...</td>\n",
       "      <td>data:image/gif;base64,R0lGODlhAQABAIAAAP//////...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1  Unnamed: 0.1.1  Unnamed: 0.1.1.1  \\\n",
       "0           0           0.0             0.0               0.0   \n",
       "\n",
       "   Unnamed: 0.1.1.1.1  Unnamed: 0.1.1.1.1.1  Unnamed: 0.1.1.1.1.1.1  index  \\\n",
       "0                 0.0                   0.0                     0.0      0   \n",
       "\n",
       "                                               title              media  \\\n",
       "0  Seattle police release video of fatal shooting...  The Seattle Times   \n",
       "\n",
       "         date  datetime                                               desc  \\\n",
       "0  01/01/2019       NaN  Faletogo died of a gunshot wound to the head, ...   \n",
       "\n",
       "                                                link  \\\n",
       "0  https://www.seattletimes.com/seattle-news/crim...   \n",
       "\n",
       "                                                 img  \n",
       "0  data:image/gif;base64,R0lGODlhAQABAIAAAP//////...  "
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=pd.read_csv('full_dataset.csv')\n",
    "df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "767df571",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert string values in date column to datetime values \n",
    "#df['date']=df['date'].apply(lambda x: datetime.strptime(str(x), '%m/%d/%Y'))\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "f597ad92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop index and img columns and unneeded columns\n",
    "df = df.drop(['index', 'img', 'datetime','Unnamed: 0','Unnamed: 0.1',\n",
    "              'Unnamed: 0.1.1','Unnamed: 0.1.1.1','Unnamed: 0.1.1.1.1',\n",
    "              'Unnamed: 0.1.1.1.1.1','Unnamed: 0.1.1.1.1.1.1','index'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "99f49677",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57051 entries, 0 to 57050\n",
      "Data columns (total 5 columns):\n",
      " #   Column  Non-Null Count  Dtype         \n",
      "---  ------  --------------  -----         \n",
      " 0   title   57051 non-null  object        \n",
      " 1   media   55081 non-null  object        \n",
      " 2   date    56761 non-null  datetime64[ns]\n",
      " 3   desc    57051 non-null  object        \n",
      " 4   link    57051 non-null  object        \n",
      "dtypes: datetime64[ns](1), object(4)\n",
      "memory usage: 2.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "8dfb3853",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fill in Missing values in 'media'\n",
    "df['media']=df.media.fillna('Others')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "a5ed87a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "570"
      ]
     },
     "execution_count": 401,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Number of unique news sources\n",
    "len(df.media.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "e07607d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "The Seattle Times           11294\n",
       "KIRO-TV                      6641\n",
       "KING 5                       3002\n",
       "MyNorthwest.com              2634\n",
       "Others                       1970\n",
       "KOMO News                    1706\n",
       "The Spokesman-Review         1651\n",
       "KOMO                         1550\n",
       "KUOW                         1466\n",
       "CHS Capitol Hill Seattle     1332\n",
       "Name: media, dtype: int64"
      ]
     },
     "execution_count": 402,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Top 10 news sources\n",
    "top_10=df.media.value_counts().head(10)\n",
    "top_10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "id": "75332b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create Local vs national media dictionary 0-local, 1 = national \"Others\" =0\n",
    "media_dict = {\n",
    "    'The Seattle Times': 0, \n",
    "       'West Seattle Blog...':0, 'Others':0 , 'CHS Capitol Hill Seattle':0,\n",
    "        'Westside Seattle':0, 'PubliCola':0, 'South Seattle Emerald':0,\n",
    "       'KIRO-TV':0,  'KOMO':0,  'Seattle PI':0,\n",
    "       'Real Change':0, 'KING 5':0,  'The Stranger':0,\n",
    "       'Council Connection':0, 'Q13 FOX':0,\n",
    "       'MyNorthwest.com':0,\n",
    "       'KOMO News':0,\n",
    "       'Seattle Met':0,   'Seattle Weekly':0,\n",
    "    'Northwest Asian Weekly':0,  'West Seattle Blog…': 0,\n",
    "    'Seattle Spectator': 0, 'Washington Times': 0,   \n",
    "    'Seattle Magazine |':0, 'Seattle Met':0,\n",
    "       'Seattle PI':0, 'Seattle Refined':0, 'Seattle Spectator':0,'South Seattle Emerald':0,\n",
    "       'Seattle Times Interactives Projects':0, 'Seattle Weekly':0,'The Seattle Globalist':0, 'The Seattle Medium':0,\n",
    "     'West Seattle Blog...':0, 'West Seattle Blog…':0, 'Westside Seattle':0, 'Real Change':0, \n",
    "    'International Examiner':0,'dailyuw.com':0\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "id": "3866eae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>media</th>\n",
       "      <th>date</th>\n",
       "      <th>desc</th>\n",
       "      <th>link</th>\n",
       "      <th>Local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seattle police release video of fatal shooting...</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Faletogo died of a gunshot wound to the head, ...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/crim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Fatal Seattle police shooting will test new ac...</td>\n",
       "      <td>Crosscut</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Faletogo was stopped by police officers while ...</td>\n",
       "      <td>https://crosscut.com/2019/01/fatal-seattle-pol...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle Police release body cam footage in fat...</td>\n",
       "      <td>MyNorthwest.com</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>The Seattle Police Department released the bod...</td>\n",
       "      <td>https://mynorthwest.com/1232749/seattle-police...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Former Seattle police chief Jim Pugel will run...</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>More than two dozen candidates are running acr...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/poli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Judge dismisses claims against 2 Seattle polic...</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>2019-01-01</td>\n",
       "      <td>Lyles had a history of contacts with SPD, most...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/crim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              media  \\\n",
       "0  Seattle police release video of fatal shooting...  The Seattle Times   \n",
       "1  Fatal Seattle police shooting will test new ac...           Crosscut   \n",
       "2  Seattle Police release body cam footage in fat...    MyNorthwest.com   \n",
       "3  Former Seattle police chief Jim Pugel will run...  The Seattle Times   \n",
       "4  Judge dismisses claims against 2 Seattle polic...  The Seattle Times   \n",
       "\n",
       "        date                                               desc  \\\n",
       "0 2019-01-01  Faletogo died of a gunshot wound to the head, ...   \n",
       "1 2019-01-01  Faletogo was stopped by police officers while ...   \n",
       "2 2019-01-01  The Seattle Police Department released the bod...   \n",
       "3 2019-01-01  More than two dozen candidates are running acr...   \n",
       "4 2019-01-01  Lyles had a history of contacts with SPD, most...   \n",
       "\n",
       "                                                link  Local  \n",
       "0  https://www.seattletimes.com/seattle-news/crim...      0  \n",
       "1  https://crosscut.com/2019/01/fatal-seattle-pol...      1  \n",
       "2  https://mynorthwest.com/1232749/seattle-police...      0  \n",
       "3  https://www.seattletimes.com/seattle-news/poli...      0  \n",
       "4  https://www.seattletimes.com/seattle-news/crim...      0  "
      ]
     },
     "execution_count": 404,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for x in df['media']:\n",
    "    if x not in media_dict:\n",
    "        media_dict[x]=1\n",
    "    \n",
    "df['Local']=[media_dict[x] for x in df['media']]\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "id": "f9498495",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'seattletimes'"
      ]
     },
     "execution_count": 405,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text =df['link'][0].split('.')[1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "6f718b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replace 'Others' in media with core website name taken from  link column\n",
    "df[df['media']=='Others']=df[df['media']=='Others'].apply(lambda x : df['link'][0].split('.')[1]  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "253c9acb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['(Burien) Blog', '10TV', '11Alive.com', '12News.com', '247Sports',\n",
       "       '6ABC', '89.7 NPR News', '9News', 'ABA Journal', 'ABC 33/40',\n",
       "       'ABC Chicago', 'ABC News', 'ABC10', 'ABC13', 'ABC4 Utah',\n",
       "       'ABC7 San Francisco', 'ABS-CBN News', 'AP News', 'Action News Jax',\n",
       "       'Action News Now', 'Akron Beacon Journal', 'Alaska Public Media',\n",
       "       'AlterNet', 'American Civil Liberties Union',\n",
       "       'American Security Today', 'Argus Leader', 'Arizona Daily Star',\n",
       "       'Army.mil', 'Ars Technica', 'Atlas Obscura', 'Auburn Examiner',\n",
       "       'Auburn Reporter', 'Austin American-Statesman', 'Axios', 'BBC',\n",
       "       'Baltimore Sun', 'Bangor Daily News', 'Bay News 9',\n",
       "       'Bellevue Reporter', 'Bellingham Herald', 'Bemidji Pioneer',\n",
       "       'Billboard', 'Biometric Update', 'Blabbermouth.net',\n",
       "       'Black Enterprise', 'Bloomberg Law', 'Bloomberg.com',\n",
       "       'Boing Boing', 'Boise State Public Radio', 'Boston 25 News',\n",
       "       'Boston Herald', 'Boston.com', 'Bothell-Kenmore Reporter',\n",
       "       'Bring Me The News', 'Brookings Institution', 'Buffalo News',\n",
       "       'Built In', 'Business - Insider', 'CBC.ca', 'CBR', 'CBS 58',\n",
       "       'CBS Boston', 'CBS Denver', 'CBS Los Angeles', 'CBS Minnesota',\n",
       "       'CBS News', 'CBS Philly', 'CBS Sacramento', 'CBS Sports',\n",
       "       'CHS Capitol Hill Seattle', 'CNET', 'CNN', 'CTV News',\n",
       "       'CTV News Vancouver', 'CalMatters', 'California Globe',\n",
       "       'Capital Journal', 'Center for American Progress',\n",
       "       'Chicago Sun-Times', 'Chicago Tonight', 'Chicago Tribune',\n",
       "       'Chico Enterprise-Record', 'Cincinnati Enquirer', 'Cinema Express',\n",
       "       'CinemaBlend', 'City & State', 'City Journal', 'Claims Journal',\n",
       "       'ClickOnDetroit', 'Columbia Missourian', 'Columbus Alive',\n",
       "       'Corvallis Gazette-Times', 'Courier-Herald', 'Courier-Journal',\n",
       "       'Courthouse News Service', 'Crime Online', 'Crosscut',\n",
       "       'DA MAN Magazine', 'DCist', 'Daily Democrat', 'Daily Express',\n",
       "       'Daily Fly', 'Daily Hive', 'Daily Mail', 'Dallas Morning News',\n",
       "       'Deadline', 'Denver Gazette', 'Denver7', 'Deseret News',\n",
       "       'Detroit Free Press', 'Detroit News', 'DroneDJ', 'EMS1', 'ESPN',\n",
       "       'East Oregonian', 'Eater DC', 'Eater LA', 'Eater Seattle',\n",
       "       'Electronic Frontier Foundation', 'Elko Daily Free Press',\n",
       "       'Ellensburg Daily Record', 'Eugene Weekly', 'Everett Herald',\n",
       "       'FBI.gov', 'FOX 10 Phoenix', 'FOX 11 Los Angeles',\n",
       "       'FOX 13 Memphis', 'FOX 2', 'FOX 5 Atlanta', 'FOX 5 DC',\n",
       "       'FOX 7 Austin', 'FOX 8', 'FOX23', 'FOX40', 'FOX43',\n",
       "       'FOX6 News Milwaukee', 'FactCheck.org', 'Federal Way Mirror',\n",
       "       'Field Gulls', 'Fingerlakes1.com', 'Firehouse Magazine',\n",
       "       'Firehouse.com', 'Forbes', 'Fox 8', 'Fox Business', 'Fox News',\n",
       "       'Fox19', 'GeekWire', 'Globe Gazette', 'GlobeNewswire',\n",
       "       'Go Grays Harbor', 'Good Housekeeping', 'Governing Magazine',\n",
       "       'Grand Forks Herald', 'Greenfield Recorder', 'Grist',\n",
       "       'GulfCoastNewsToday.com', 'Gwinnett Daily Post', 'Halo Hangout',\n",
       "       \"Harper's Bazaar\", 'Hawaii Tribune-Herald', 'Heavy.com',\n",
       "       'Henry Herald', 'Herald & Review', 'Homeland Security Today',\n",
       "       'Honolulu Civil Beat', 'Honolulu Star-Advertiser', 'Hoodline',\n",
       "       'Houston Chronicle', 'Hunter College NYC Food Policy Center',\n",
       "       'Hyperallergic', 'ICE', 'INFORUM', 'IPWatchdog.com', 'Idaho Press',\n",
       "       'Ignatian Solidarity Network', 'IndyStar', 'Inkstick Media',\n",
       "       'Inlander', 'Insider', 'International Examiner', 'InvestigateWest',\n",
       "       'Irish America Magazine', 'Issaquah Reporter', 'Jacobin',\n",
       "       'Juneau Empire', 'Just The News', 'KALB', 'KARE 11', 'KATU',\n",
       "       'KATV', 'KCEN-TV', 'KCRA', 'KCRG', 'KCTV5', 'KDNL', 'KDVR', 'KEPR',\n",
       "       'KERA News', 'KFOR.com', 'KGMI', 'KGW', 'KHQ.com', 'KIMA',\n",
       "       'KING 5', 'KING5.com', 'KIRO-TV', 'KJCT', 'KLEW', 'KMTR', 'KNDU',\n",
       "       'KNKX', 'KOLR', 'KOMO', 'KOMO News', 'KPBS', 'KPQ', 'KPTV', 'KRCG',\n",
       "       'KREM', 'KREM.com', 'KRON4', 'KSAT.com', 'KSDK', 'KTHV', 'KTLA',\n",
       "       'KTVL', 'KULR-8', 'KUOW', 'KUTV', 'KVAL', 'KVI', 'KVUE', 'KWCH',\n",
       "       'KWKT - FOX 44', 'KWQC', 'KXAN Austin', 'KXLY', 'Kent Reporter',\n",
       "       'Kentucky Center for Investigative Reporting', 'King 5',\n",
       "       'Kirkland Reporter', 'Kitsap Sun', 'LGBTQ Nation', 'LSE Blogs',\n",
       "       'Labor Notes |', 'Las Vegas Review-Journal', 'Law & Crime',\n",
       "       'Law Officer', 'Liberation News', 'Lisa Herbold',\n",
       "       'Living Snoqualmie', 'Los Angeles Daily News', 'Los Angeles Times',\n",
       "       'Loudwire', 'Lubbock Avalanche-Journal', 'MEAWW', 'MLB.com',\n",
       "       'MLive.com', 'MYEVERETTNEWS.com', 'Macon Telegraph',\n",
       "       'Madison Park Times', 'Madison.com', 'Magnolia Reporter',\n",
       "       'Marijuana Moment', 'Marketplace.org', 'Marysville Globe',\n",
       "       'Maui News', 'Mercer Island Reporter', 'MetalCastle',\n",
       "       'Metro Weekly', 'MetroWest Daily News', 'Miami Dolphins',\n",
       "       'Michigan Radio', 'Military Times', 'Milwaukee Journal Sentinel',\n",
       "       'MinnPost', 'Moguldom', 'Mother Jones', 'My Ballard',\n",
       "       'My Edmonds News', 'MyNorthwest.com', 'NBC 5 Dallas-Fort Worth',\n",
       "       'NBC 7 San Diego', 'NBC Chicago', 'NBC Connecticut',\n",
       "       'NBC New York', 'NBC News', 'NBC Right Now', 'NBC Sports',\n",
       "       'NBC4 WCMH-TV', 'NCWLIFE', 'NFL.com', 'NHL.com', 'NPR',\n",
       "       'NWI Times', 'Nashville.gov', 'National University', 'Nature',\n",
       "       'New Haven Independent', 'New Haven Register', 'New York Post',\n",
       "       'News 1130', 'News 9', 'News @ Northeastern',\n",
       "       'News Apps — ProPublica', 'News on 6', 'Newsweek', 'Next City',\n",
       "       'Normandy Park Blog', 'North County Outlook',\n",
       "       'Northeast Indiana Public Radio', 'Northwest Asian Weekly',\n",
       "       'Northwest Progressive Institute', 'OPB', 'Officer', 'OneZero',\n",
       "       'Oregon Live', 'Oregon Public Broadcasting', 'Oxygen',\n",
       "       'PR Newswire', 'Pacific Standard', 'Palm Beach Post', 'Patch',\n",
       "       'People.com', 'Philadelphia Inquirer', 'Phoenix New Times',\n",
       "       'Phys.org', 'Pittsburgh Post-Gazette', 'Politico', 'Polygon',\n",
       "       'Pop Culture', 'Port Townsend Leader', 'Portland Mercury',\n",
       "       'ProPublica', 'PubliCola', 'Q13 FOX', 'Quartz',\n",
       "       'Queen Anne & Magnolia News', 'Radar Online', 'Real Change',\n",
       "       'Reason Magazine', 'Redmond Reporter', 'Renton Reporter',\n",
       "       'Reporter Newspapers',\n",
       "       'Reporters Committee for Freedom of the Press', 'Reuters',\n",
       "       'Reveal | from The Center for Investigative Reporting',\n",
       "       'Robert Feder', 'RochesterFirst', 'Rolling Stone', 'Romper',\n",
       "       'SDOT Blog', 'SFGATE', 'SFist', 'SPD Blotter', 'Salem Reporter',\n",
       "       'San Francisco Chronicle', 'Santa Fe New Mexican', 'Screen Rant',\n",
       "       'Seacoastonline.com', 'Seattle Magazine |', 'Seattle Met',\n",
       "       'Seattle PI', 'Seattle Refined', 'Seattle Spectator',\n",
       "       'Seattle Times Interactives Projects', 'Seattle Weekly',\n",
       "       'Sentinel Colorado', 'SiliconANGLE', 'Slate', 'Smart Cities World',\n",
       "       'Snopes.com', 'South Bend Tribune', 'South Seattle Emerald',\n",
       "       'SouthPasadenan.com', 'Southwest Journal', 'Spectrum News',\n",
       "       'Sports Illustrated', 'Springfield News-Sun',\n",
       "       'St. Louis Post-Dispatch', 'St. Pete Catalyst',\n",
       "       'St. Thomas Source', 'Stamford Advocate', 'Star Tribune',\n",
       "       'StateScoop', 'Sun Sentinel', 'TMJ4', 'TXK Today',\n",
       "       'Tacoma News Tribune', 'Tallahassee Democrat',\n",
       "       'Tampa Bay Buccaneers', 'Tampa Bay Times', 'Tech Times',\n",
       "       'TechCrunch', 'The 74', 'The Advocate', 'The Appeal',\n",
       "       'The Arizona Republic', 'The Arkansas Democrat-Gazette',\n",
       "       'The Atlanta Journal-Constitution', 'The Atlantic',\n",
       "       'The Bay State Banner', 'The Boston Globe',\n",
       "       'The Business Journals', 'The Center Square', 'The Chronicle',\n",
       "       'The Coloradoan', 'The Columbian', 'The Columbus Dispatch',\n",
       "       'The Commercial Appeal', 'The Daily Advertiser', 'The Daily Beast',\n",
       "       'The Daily News', 'The Daily Orange', 'The Daily World',\n",
       "       'The Denver Post', 'The Des Moines Register', 'The Desert Sun',\n",
       "       'The Drive', 'The Eatonville Dispatch', 'The Economist',\n",
       "       'The Epoch Times', 'The Federalist', 'The Fort Morgan Times',\n",
       "       'The France 24 Observers', 'The Fresno Bee', 'The Guam Daily Post',\n",
       "       'The Guardian', 'The Harvard Crimson', 'The Hechinger Report',\n",
       "       'The Heritage Foundation', 'The Independent', 'The Intercept',\n",
       "       'The Ithaca Voice', 'The Jerusalem Post',\n",
       "       \"The Journalist's Resource\", 'The Maritime Executive',\n",
       "       'The Marshall Project', 'The Mercury News', 'The Nation',\n",
       "       'The New York Times', 'The New Yorker', 'The Northern Light',\n",
       "       'The Oklahoman', 'The Olympian', 'The Pew Charitable Trusts',\n",
       "       'The Philadelphia Citizen', 'The Pilot', 'The Post Millennial',\n",
       "       'The Press Democrat', 'The Province', 'The Quad City Times',\n",
       "       'The Register Guard', 'The Root', 'The Sacramento Bee',\n",
       "       'The Salt Lake Tribune', 'The San Diego Union-Tribune',\n",
       "       'The SeaTac Blog', 'The Seattle Globalist', 'The Seattle Medium',\n",
       "       'The Seattle Times', 'The Spokesman-Review', 'The Spun',\n",
       "       'The Stanford Daily', 'The Stranger', 'The Suburban Times',\n",
       "       'The Tennessean', 'The Texan', 'The Texas Observer',\n",
       "       'The Texas Tribune', 'The Topeka Capital-Journal',\n",
       "       'The Tukwila Blog', 'The Verge', 'The Wall Street Journal',\n",
       "       'The Washington Post', 'The Waterland Blog', 'The Week',\n",
       "       'The Westerly Sun', 'The World', 'TheBlaze', 'TheGamer', 'TheHill',\n",
       "       'TheSpectrum.com', 'ThinkProgress',\n",
       "       'ThisWeek Community News | The Columbus Dispatch', 'Thrillist',\n",
       "       'Time Magazine', 'Tri-City Herald', 'True Crime Daily',\n",
       "       'Tukwila Reporter', 'Tulsa World', 'UPI', 'USA Today',\n",
       "       'USNews.com', 'Under The Radar Magazine', 'Union-Bulletin',\n",
       "       'University of Washington', 'Unmanned Aerial', 'VICE',\n",
       "       'Vacaville Reporter', 'Vashon Beachcomber', 'Ventura County Star',\n",
       "       'Vox', 'WABI', 'WAMU', 'WAVY.com', 'WBEZ Interactive', 'WCBD',\n",
       "       'WCCO | CBS Minnesota', 'WFAA', 'WFLA', 'WGME', 'WGN-TV', 'WGRZ',\n",
       "       'WHAM', 'WHP', 'WHYY', 'WINA', 'WIRED', 'WJHL', 'WJLA', 'WKRG',\n",
       "       'WKRN', 'WKYC', 'WLUK', 'WOSU', 'WPLG Local 10', 'WPMI', 'WPXI',\n",
       "       'WSAV-TV', 'WSB-TV', 'WSET', 'WSTM', 'WSYX', 'WTHR', 'WTOC',\n",
       "       'WTRF', 'WTSP.com', 'WTTW News', 'WWAY', 'WWLP', 'WWNY', 'WZTV',\n",
       "       'Wall Street Journal', 'Washington Times', 'Watauga Democrat',\n",
       "       'WebMD', 'Weedmaps', 'West Seattle Blog...', 'West Seattle Blog…',\n",
       "       'Westside Seattle', 'Woodinville Weekly', 'YES! Magazine', 'Yahoo',\n",
       "       'Yahoo News', 'Yahoo! Sports', 'YakTriNews.com', 'Yakima Herald',\n",
       "       'York Daily Record', 'YourValley.net', 'amNewYork', 'austonia',\n",
       "       'dailyuw.com', 'goSkagit', 'iFIBER One', 'iHeartRadio',\n",
       "       'iLoveKent', 'seattletimes', '| The Stanly News & Press'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 407,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(df.media.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "id": "4b3667e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>media</th>\n",
       "      <th>date</th>\n",
       "      <th>desc</th>\n",
       "      <th>link</th>\n",
       "      <th>Local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [title, media, date, desc, link, Local]\n",
       "Index: []"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df['media']=='Others']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 409,
   "id": "c9082733",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 57051 entries, 0 to 57050\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   57051 non-null  object\n",
      " 1   media   57051 non-null  object\n",
      " 2   date    56770 non-null  object\n",
      " 3   desc    57051 non-null  object\n",
      " 4   link    57051 non-null  object\n",
      " 5   Local   57051 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 2.6+ MB\n"
     ]
    }
   ],
   "source": [
    "#Checking for missing data\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "id": "a1f72203",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dropping rows with missing date value\n",
    "df=df.dropna(subset=['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "9bc34a9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 56770 entries, 0 to 57050\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   title   56770 non-null  object\n",
      " 1   media   56770 non-null  object\n",
      " 2   date    56770 non-null  object\n",
      " 3   desc    56770 non-null  object\n",
      " 4   link    56770 non-null  object\n",
      " 5   Local   56770 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 3.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "id": "151a523a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>media</th>\n",
       "      <th>date</th>\n",
       "      <th>desc</th>\n",
       "      <th>link</th>\n",
       "      <th>Local</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Seattle police release video of fatal shooting...</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>Faletogo died of a gunshot wound to the head, ...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/crim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Seattle Police release body cam footage in fat...</td>\n",
       "      <td>MyNorthwest.com</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>The Seattle Police Department released the bod...</td>\n",
       "      <td>https://mynorthwest.com/1232749/seattle-police...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Former Seattle police chief Jim Pugel will run...</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>More than two dozen candidates are running acr...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/poli...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Judge dismisses claims against 2 Seattle polic...</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>Lyles had a history of contacts with SPD, most...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/crim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>‘We have your DNA’: Seattle police create vide...</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>2019-01-01 00:00:00</td>\n",
       "      <td>We have your DNA. It is in your best interest ...</td>\n",
       "      <td>https://www.seattletimes.com/seattle-news/crim...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              media  \\\n",
       "0  Seattle police release video of fatal shooting...  The Seattle Times   \n",
       "2  Seattle Police release body cam footage in fat...    MyNorthwest.com   \n",
       "3  Former Seattle police chief Jim Pugel will run...  The Seattle Times   \n",
       "4  Judge dismisses claims against 2 Seattle polic...  The Seattle Times   \n",
       "5  ‘We have your DNA’: Seattle police create vide...  The Seattle Times   \n",
       "\n",
       "                  date                                               desc  \\\n",
       "0  2019-01-01 00:00:00  Faletogo died of a gunshot wound to the head, ...   \n",
       "2  2019-01-01 00:00:00  The Seattle Police Department released the bod...   \n",
       "3  2019-01-01 00:00:00  More than two dozen candidates are running acr...   \n",
       "4  2019-01-01 00:00:00  Lyles had a history of contacts with SPD, most...   \n",
       "5  2019-01-01 00:00:00  We have your DNA. It is in your best interest ...   \n",
       "\n",
       "                                                link Local  \n",
       "0  https://www.seattletimes.com/seattle-news/crim...     0  \n",
       "2  https://mynorthwest.com/1232749/seattle-police...     0  \n",
       "3  https://www.seattletimes.com/seattle-news/poli...     0  \n",
       "4  https://www.seattletimes.com/seattle-news/crim...     0  \n",
       "5  https://www.seattletimes.com/seattle-news/crim...     0  "
      ]
     },
     "execution_count": 412,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df=df[df['Local']==0]\n",
    "df=df.drop_duplicates(subset=['title','desc','link','media'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "15fcef7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2891, 6)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df=df\n",
    "df.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "ec80ef7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['The Seattle Times', 'MyNorthwest.com', 'KING 5', 'KIRO-TV',\n",
       "       'Q13 FOX', 'Seattle Weekly', 'CHS Capitol Hill Seattle',\n",
       "       'West Seattle Blog...', 'Seattle PI', 'Northwest Asian Weekly',\n",
       "       'Real Change', 'KOMO', 'The Stranger', 'KOMO News',\n",
       "       'Westside Seattle', 'West Seattle Blog…', 'International Examiner',\n",
       "       'Seattle Spectator', 'Washington Times', 'South Seattle Emerald',\n",
       "       'Seattle Times Interactives Projects', 'The Seattle Globalist',\n",
       "       'dailyuw.com', 'Seattle Magazine |', 'Seattle Refined',\n",
       "       'PubliCola', 'Seattle Met', 'The Seattle Medium'], dtype=object)"
      ]
     },
     "execution_count": 282,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "media=df['media'][df['Local']==0].unique()\n",
    "media"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "bdb8147b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CHS Capitol Hill Seattle', 'International Examiner', 'KING 5',\n",
       "       'KIRO-TV', 'KOMO', 'KOMO News', 'MyNorthwest.com',\n",
       "       'Northwest Asian Weekly', 'PubliCola', 'Q13 FOX', 'Real Change',\n",
       "       'Seattle Magazine |', 'Seattle Met', 'Seattle PI',\n",
       "       'Seattle Refined', 'Seattle Spectator',\n",
       "       'Seattle Times Interactives Projects', 'Seattle Weekly',\n",
       "       'South Seattle Emerald', 'The Seattle Globalist',\n",
       "       'The Seattle Medium', 'The Seattle Times', 'The Stranger',\n",
       "       'Washington Times', 'West Seattle Blog...', 'West Seattle Blog…',\n",
       "       'Westside Seattle', 'dailyuw.com'], dtype=object)"
      ]
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#All Media Sources\n",
    "np.sort(df.media.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "9046bbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "#Tokenize and Normalize (standardize) the text by removing all punctuation \n",
    "#from 'title' and 'desc' making all text lower case\n",
    "\n",
    "df.title = df.title.apply(lambda x: nltk.RegexpTokenizer(r\"\\w+\").tokenize(x.lower()))\n",
    "df.desc = df.desc.apply(lambda x: nltk.RegexpTokenizer(r\"\\w+\").tokenize(x.lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "c58b3402",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extend list of stopwords to include search terms\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "newStopWords = ['seattle','police','officer','officers', 'shooting', 'killed', 'spd', 'department', 'said', 'say','call']\n",
    "#'dependable','professional','respectful','delivering','quality','enforces','prevents']\n",
    "stopwords.extend(newStopWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "7a68148f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove Stop words from the text and lemmetize/text stemming\n",
    "#ps = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "'''df.title = df.title.apply(lambda x: [ps.stem(word) for word in x if word not in stopwords])\n",
    "df.desc = df.desc.apply(lambda x: [ps.stem(word) for word in x if word not in stopwords])'''\n",
    "\n",
    "df.title = df.title.apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in stopwords])\n",
    "df.desc = df.desc.apply(lambda x: [lemmatizer.lemmatize(word) for word in x if word not in stopwords])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "4c5bdba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#create trigram from tokenized list\n",
    "from nltk.util import ngrams\n",
    "def ngramconvert(df,n=3):\n",
    "    for item in df[['title', 'desc']].columns:\n",
    "        df['newtri'+item]=df[item].apply(lambda token_list: list(ngrams(token_list, n)))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "d32a07a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ngramconvert(df, n=3)\n",
    "\n",
    "#Initialize FreqDist objects\n",
    "fdist_title = FreqDist()\n",
    "fdist_desc = FreqDist()\n",
    "\n",
    "#Measure of the frequency of words in the 'title' column\n",
    "for lists_ in df['title']:\n",
    "    for words in lists_:\n",
    "        fdist_title[words]+=1\n",
    "        \n",
    "title_word_freq = pd.Series(fdist_title, name='freq_title').sort_values(ascending=False)\n",
    "\n",
    "#Measure of the frequency of words in the 'desc' column\n",
    "for lists_ in df['desc']:\n",
    "    for words in lists_:\n",
    "        fdist_desc[words]+=1\n",
    "        \n",
    "desc_word_freq = pd.Series(fdist_desc, name='freq_desc').sort_values(ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "490bdb25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(               freq_desc  freq_title\n",
       " city                 356       204.0\n",
       " year                 291       157.0\n",
       " according            261         2.0\n",
       " man                  224       392.0\n",
       " chief                170       110.0\n",
       " county               157        95.0\n",
       " council              150       125.0\n",
       " old                  148        86.0\n",
       " two                  142        47.0\n",
       " arrested             141       139.0\n",
       " one                  137        26.0\n",
       " new                  135       105.0\n",
       " people               133        49.0\n",
       " investigation        132        45.0\n",
       " report               125        55.0\n",
       " crime                124        85.0\n",
       " time                 123        27.0\n",
       " last                 118        12.0\n",
       " shot                 117       111.0\n",
       " suspect              111       158.0,\n",
       "                freq_desc  freq_title\n",
       " man                  224       392.0\n",
       " city                 356       204.0\n",
       " suspect              111       158.0\n",
       " year                 291       157.0\n",
       " arrested             141       139.0\n",
       " council              150       125.0\n",
       " shot                 117       111.0\n",
       " chief                170       110.0\n",
       " new                  135       105.0\n",
       " county               157        95.0\n",
       " old                  148        86.0\n",
       " crime                124        85.0\n",
       " report               125        55.0\n",
       " people               133        49.0\n",
       " two                  142        47.0\n",
       " investigation        132        45.0\n",
       " time                 123        27.0\n",
       " one                  137        26.0\n",
       " last                 118        12.0\n",
       " according            261         2.0)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Calculating the top 50 words \n",
    "word_freq_table = pd.DataFrame(desc_word_freq).join(title_word_freq)\n",
    "\n",
    "#word_freq_table['freq_total']= word_freq_table['freq_desc']+word_freq_table['freq_title']\n",
    "word_freq_table = word_freq_table.fillna(int(0))\n",
    "word_freq_table.head(20).sort_values(by=['freq_desc','freq_title'], ascending=False), word_freq_table.head(20).sort_values(by=['freq_title','freq_desc'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "26379cac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Trigram for news headlines\n",
    "trigram_list =[]\n",
    "\n",
    "for trigrams_ in df['newtrititle']:\n",
    "    trigram_list.append(trigrams_)\n",
    "trigram_headlines =pd.DataFrame(trigram_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "fe439c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_headlines =trigram_headlines.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "80f518c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_headlines =pd.DataFrame(trigram_headlines)\n",
    "trigram_headlines.columns = ['trigram_headlines']\n",
    "trigram_headlines=trigram_headlines.reset_index().drop(['level_0', 'level_1'], axis=1)\n",
    "trigram_headlines= trigram_headlines[trigram_headlines['trigram_headlines']!='None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "ad2c514b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trigram_headlines                   \n",
       "(mayor, jenny, durkan)                  20\n",
       "(year, old, man)                        14\n",
       "(cal, anderson, park)                   14\n",
       "(west, crime, watch)                    14\n",
       "(black, life, matter)                   12\n",
       "(man, fatally, shot)                    11\n",
       "(washington, state, nation)             11\n",
       "(year, old, girl)                       11\n",
       "(daily, news, update)                   10\n",
       "(covid, 19, area)                       10\n",
       "(chief, carmen, best)                   10\n",
       "(coronavirus, daily, news)              10\n",
       "(today, covid, 19)                      10\n",
       "(area, washington, state)               10\n",
       "(19, area, washington)                  10\n",
       "(know, today, covid)                    10\n",
       "(city, council, district)                8\n",
       "(city, council, vote)                    8\n",
       "(year, old, boy)                         8\n",
       "(19, year, old)                          7\n",
       "(fatally, shoot, man)                    7\n",
       "(king, county, sheriff)                  6\n",
       "(18, year, old)                          6\n",
       "(year, old, woman)                       6\n",
       "(chinatown, international, district)     6\n",
       "(3, year, old)                           6\n",
       "(news, update, march)                    6\n",
       "(city, council, member)                  6\n",
       "(catalytic, converter, theft)            6\n",
       "(11, year, old)                          6\n",
       "dtype: int64"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_headlines.value_counts().head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "0dbf815c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Trigram for news article summaries\n",
    "trigram_sumlist =[]\n",
    "\n",
    "for trigrams_ in df['newtridesc']:\n",
    "    trigram_sumlist.append(trigrams_)\n",
    "trigram_summaries =pd.DataFrame(trigram_sumlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "cb26e155",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_summaries =trigram_summaries.stack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "fe0e4933",
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_summaries =pd.DataFrame(trigram_summaries)\n",
    "trigram_summaries.columns = ['trigram_news_summaries']\n",
    "trigram_summaries=trigram_summaries.reset_index().drop(['level_0', 'level_1'], axis=1)\n",
    "trigram_summaries= trigram_summaries[trigram_summaries['trigram_news_summaries']!='None']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "d094931e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "trigram_news_summaries       \n",
       "(mayor, jenny, durkan)           50\n",
       "(chief, carmen, best)            35\n",
       "(year, old, man)                 34\n",
       "(chief, adrian, diaz)            29\n",
       "(county, sheriff, office)        23\n",
       "(interim, chief, adrian)         20\n",
       "(black, life, matter)            19\n",
       "(city, council, member)          18\n",
       "(king, county, sheriff)          17\n",
       "(line, 206, 233)                 15\n",
       "(206, 233, 5000)                 15\n",
       "(tip, line, 206)                 15\n",
       "(harborview, medical, center)    14\n",
       "(city, council, voted)           12\n",
       "(photo, courtesy, layoff)        12\n",
       "(courtesy, layoff, todd)         11\n",
       "(layoff, todd, herman)           11\n",
       "(7, 9, 2021)                     10\n",
       "(19, year, old)                  10\n",
       "(note, 7, 9)                     10\n",
       "(herman, show, note)             10\n",
       "(9, 2021, interim)               10\n",
       "(todd, herman, show)             10\n",
       "(cox, medium, group)             10\n",
       "(washington, state, patrol)      10\n",
       "(show, note, 7)                  10\n",
       "(2021, interim, chief)            9\n",
       "(body, camera, video)             9\n",
       "(violent, crime, tip)             9\n",
       "(anyone, information, asked)      9\n",
       "dtype: int64"
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trigram_summaries.value_counts().head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302fa40d",
   "metadata": {},
   "source": [
    "An initial analysis of the trigram_summaries results (shown above) and the word frequency distribution table for the news sumarries indicates that there is a set a text that appears repeatedly for 2,254 times and it contains text. \n",
    "\n",
    "Upon further investigation, we were able to identify the source text on the Seattle Police Department's website:\n",
    "\n",
    "\"The Seattle Police Department (SPD) prevents crime, enforces laws, and supports quality public safety by delivering respectful, professional, and dependable police services. SPD operates within a framework that divides the city into five geographical areas called \"precincts\". These precincts define east, west, north, south, and southwest patrol areas, with a police station in each.\"\n",
    "\n",
    "This means that 2,254 articles in the dataset are coming from SPD or from a https://www.seattle.gov/police site which have the above text at the bottom of those pages.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b3e644f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save clean, preprocessed dataframe to csv\n",
    "df.to_csv('news_clean_tokens.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "ac0a8081",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: wordcloud in /opt/conda/lib/python3.8/site-packages (1.8.1)\n",
      "Requirement already satisfied: matplotlib in /opt/conda/lib/python3.8/site-packages (from wordcloud) (3.4.2)\n",
      "Requirement already satisfied: numpy>=1.6.1 in /opt/conda/lib/python3.8/site-packages (from wordcloud) (1.20.3)\n",
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (from wordcloud) (8.3.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.8.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (0.10.0)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (2.4.7)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib->wordcloud) (1.3.1)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.8/site-packages (from cycler>=0.10->matplotlib->wordcloud) (1.15.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f5afd21e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pillow in /opt/conda/lib/python3.8/site-packages (8.3.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "af39a848",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37724a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Create and generate a word cloud image:\n",
    "spd_mask = np.array(Image.open('spd.png'))\n",
    "wc = WordCloud(background_color='white',mask=spd_mask, max_words=50)\n",
    "\n",
    "# for News summaries\n",
    "wc.generate(str(df.desc))\n",
    "\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize = (16,9))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d79f2ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For News Headlines\n",
    "wc.generate(str(df.title))\n",
    "\n",
    "\n",
    "# Display the generated image:\n",
    "plt.figure(figsize = (16,9))\n",
    "plt.imshow(wc, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573eaef4",
   "metadata": {},
   "source": [
    "## Topic Modeling "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "f3a0b6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyLDAvis in /opt/conda/lib/python3.8/site-packages (3.3.1)\n",
      "Requirement already satisfied: sklearn in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (0.0)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (0.18.2)\n",
      "Requirement already satisfied: gensim in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (4.0.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (2.11.3)\n",
      "Requirement already satisfied: joblib in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (1.0.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (0.24.2)\n",
      "Requirement already satisfied: numexpr in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (2.7.3)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (1.7.0)\n",
      "Requirement already satisfied: funcy in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (1.16)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (1.20.3)\n",
      "Requirement already satisfied: pandas>=1.2.0 in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (1.2.5)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.8/site-packages (from pyLDAvis) (49.6.0.post20210108)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2.8.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=1.2.0->pyLDAvis) (2021.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7.3->pandas>=1.2.0->pyLDAvis) (1.15.0)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /opt/conda/lib/python3.8/site-packages (from gensim->pyLDAvis) (5.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /opt/conda/lib/python3.8/site-packages (from jinja2->pyLDAvis) (1.1.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.8/site-packages (from scikit-learn->pyLDAvis) (2.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pyLDAvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "b5265f7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for LDA\n",
    "import pyLDAvis\n",
    "\n",
    "\n",
    "# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "62d1ee1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>media</th>\n",
       "      <th>desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[release, video, fatal, new, year, eve]</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>[faletogo, died, gunshot, wound, head, medical...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[release, body, cam, footage, fatal, aurora, a...</td>\n",
       "      <td>MyNorthwest.com</td>\n",
       "      <td>[released, body, cam, footage, involving, car,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[former, chief, jim, pugel, run, city, council]</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>[two, dozen, candidate, running, across, seven...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[judge, dismisses, claim, 2, fatal, charleena,...</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>[lyles, history, contact, call, report, domest...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[dna, create, video, get, killer, come, forward]</td>\n",
       "      <td>The Seattle Times</td>\n",
       "      <td>[dna, best, interest, contact, u, immediately,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               title              media  \\\n",
       "0            [release, video, fatal, new, year, eve]  The Seattle Times   \n",
       "2  [release, body, cam, footage, fatal, aurora, a...    MyNorthwest.com   \n",
       "3    [former, chief, jim, pugel, run, city, council]  The Seattle Times   \n",
       "4  [judge, dismisses, claim, 2, fatal, charleena,...  The Seattle Times   \n",
       "5   [dna, create, video, get, killer, come, forward]  The Seattle Times   \n",
       "\n",
       "                                                desc  \n",
       "0  [faletogo, died, gunshot, wound, head, medical...  \n",
       "2  [released, body, cam, footage, involving, car,...  \n",
       "3  [two, dozen, candidate, running, across, seven...  \n",
       "4  [lyles, history, contact, call, report, domest...  \n",
       "5  [dna, best, interest, contact, u, immediately,...  "
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_topic =df[['title', 'media', 'desc']]\n",
    "df_topic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "bdeb217d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "headline_dictionary = corpora.Dictionary(df_topic.title)\n",
    "\n",
    "# Create Corpus\n",
    "headline_texts = df_topic['title']\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [headline_dictionary.doc2bow(text) for text in headline_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "65b12bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "summary_dictionary = corpora.Dictionary(df_topic.title)\n",
    "\n",
    "# Create Corpus\n",
    "summary_texts = df_topic['desc']\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus_sum = [summary_dictionary.doc2bow(text) for text in summary_texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "ebebe976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Build LDA model for headlines\n",
    "lda_model_headline = gensim.models.LdaMulticore(corpus=corpus,\n",
    "                                       id2word=headline_dictionary,\n",
    "                                       num_topics=num_topics,\n",
    "                                       random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "71ba6933",
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of topics\n",
    "num_topics = 5\n",
    "\n",
    "# Build LDA model for summaries\n",
    "lda_model_summary = gensim.models.LdaMulticore(corpus=corpus_sum,\n",
    "                                       id2word=summary_dictionary,\n",
    "                                       num_topics=num_topics,\n",
    "                                       random_state =42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "dab9ad8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "b2dbb90b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.016*\"man\" + 0.009*\"chief\" + 0.008*\"city\" + 0.007*\"year\" + 0.007*\"arrest\" '\n",
      "  '+ 0.007*\"video\" + 0.006*\"suspect\" + 0.005*\"death\" + 0.005*\"missing\" + '\n",
      "  '0.005*\"council\"'),\n",
      " (1,\n",
      "  '0.016*\"man\" + 0.013*\"woman\" + 0.011*\"shot\" + 0.010*\"city\" + 0.007*\"year\" + '\n",
      "  '0.005*\"fatally\" + 0.005*\"death\" + 0.005*\"park\" + 0.005*\"precinct\" + '\n",
      "  '0.005*\"video\"'),\n",
      " (2,\n",
      "  '0.014*\"man\" + 0.011*\"city\" + 0.011*\"council\" + 0.009*\"2\" + 0.006*\"protest\" '\n",
      "  '+ 0.005*\"new\" + 0.005*\"arrested\" + 0.005*\"dead\" + 0.004*\"say\" + '\n",
      "  '0.004*\"mayor\"'),\n",
      " (3,\n",
      "  '0.021*\"man\" + 0.012*\"arrested\" + 0.010*\"suspect\" + 0.007*\"west\" + '\n",
      "  '0.006*\"protest\" + 0.006*\"year\" + 0.005*\"car\" + 0.005*\"fire\" + 0.005*\"city\" '\n",
      "  '+ 0.005*\"5\"'),\n",
      " (4,\n",
      "  '0.008*\"man\" + 0.007*\"year\" + 0.006*\"chief\" + 0.006*\"arrested\" + '\n",
      "  '0.006*\"durkan\" + 0.006*\"mayor\" + 0.006*\"protest\" + 0.005*\"suspect\" + '\n",
      "  '0.005*\"crime\" + 0.005*\"1\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model_headline.print_topics())\n",
    "doc_lda_headline = lda_model_headline[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "3943e6ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0,\n",
      "  '0.010*\"city\" + 0.010*\"year\" + 0.010*\"last\" + 0.008*\"chief\" + 0.008*\"man\" + '\n",
      "  '0.006*\"according\" + 0.006*\"investigation\" + 0.005*\"video\" + 0.005*\"council\" '\n",
      "  '+ 0.004*\"federal\"'),\n",
      " (1,\n",
      "  '0.015*\"city\" + 0.009*\"shot\" + 0.007*\"year\" + 0.006*\"woman\" + 0.006*\"man\" + '\n",
      "  '0.006*\"investigation\" + 0.006*\"park\" + 0.006*\"precinct\" + 0.005*\"east\" + '\n",
      "  '0.005*\"wednesday\"'),\n",
      " (2,\n",
      "  '0.015*\"city\" + 0.011*\"council\" + 0.006*\"people\" + 0.006*\"year\" + '\n",
      "  '0.005*\"county\" + 0.005*\"two\" + 0.005*\"new\" + 0.005*\"man\" + 0.005*\"arrested\" '\n",
      "  '+ 0.005*\"crime\"'),\n",
      " (3,\n",
      "  '0.015*\"according\" + 0.011*\"year\" + 0.010*\"man\" + 0.008*\"arrested\" + '\n",
      "  '0.007*\"old\" + 0.007*\"reported\" + 0.006*\"night\" + 0.006*\"one\" + 0.006*\"city\" '\n",
      "  '+ 0.005*\"two\"'),\n",
      " (4,\n",
      "  '0.010*\"year\" + 0.010*\"chief\" + 0.009*\"according\" + 0.009*\"city\" + '\n",
      "  '0.007*\"mayor\" + 0.007*\"crime\" + 0.006*\"durkan\" + 0.006*\"old\" + '\n",
      "  '0.005*\"county\" + 0.005*\"two\"')]\n"
     ]
    }
   ],
   "source": [
    "pprint(lda_model_summary.print_topics())\n",
    "doc_lda_summary = lda_model_summary[corpus_sum]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "729fec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "98c8642e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model = Word2Vec(df['desc'], min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "ccfa7f65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('asked', 0.8727303743362427),\n",
       " ('state', 0.8665550351142883),\n",
       " ('east', 0.8660399913787842),\n",
       " ('information', 0.8654963970184326),\n",
       " ('part', 0.8646700382232666),\n",
       " ('car', 0.8644542694091797),\n",
       " ('report', 0.8643311858177185),\n",
       " ('patrol', 0.8640473484992981),\n",
       " ('shot', 0.8638507723808289),\n",
       " ('public', 0.8637387156486511)]"
      ]
     },
     "execution_count": 316,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_model.wv.most_similar('asian', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f5bc16d0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('line', 0.7935531139373779),\n",
       " ('march', 0.793512761592865),\n",
       " ('2017', 0.7893218398094177),\n",
       " ('defund', 0.7868725061416626),\n",
       " ('000', 0.7865320444107056),\n",
       " ('50', 0.786282479763031),\n",
       " ('budget', 0.7857242822647095),\n",
       " ('got', 0.7855495810508728),\n",
       " ('another', 0.7848026752471924),\n",
       " ('patrol', 0.7844415307044983)]"
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_model.wv.most_similar('white', topn=10)  # - The word 'immigrant' is not present in the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "id": "da8ab034",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('assist', 0.5442954897880554),\n",
       " ('opa', 0.4687732458114624),\n",
       " ('pay', 0.46356847882270813),\n",
       " ('friend', 0.4597417414188385),\n",
       " ('tear', 0.45627257227897644),\n",
       " ('minneapolis', 0.45361268520355225),\n",
       " ('meeting', 0.4522327780723572),\n",
       " ('policing', 0.44754737615585327),\n",
       " ('please', 0.4461442530155182),\n",
       " ('dozen', 0.4417855143547058)]"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_model.wv.most_similar('african', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "id": "b5e9b515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('coordinating', 0.3692871928215027),\n",
       " ('ronald', 0.35857197642326355),\n",
       " ('government', 0.3223722577095032),\n",
       " ('nicole', 0.32027468085289),\n",
       " ('administer', 0.3177845776081085),\n",
       " ('surviving', 0.3117476999759674),\n",
       " ('4333', 0.30967745184898376),\n",
       " ('hetle', 0.3064630627632141),\n",
       " ('8th', 0.3059508502483368),\n",
       " ('somewhere', 0.3052448630332947)]"
      ]
     },
     "execution_count": 319,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_model.wv.most_similar('spanish', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6c50b1",
   "metadata": {},
   "source": [
    "## Vectorizing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "384f3a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorize model\n",
    "vectors_wv = cluster_model.wv.vectors\n",
    "indicies_wv = {word:cluster_model.wv.get_vector(word, norm=True) for word in cluster_model.wv.key_to_index}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 321,
   "id": "6a7775cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize(line): \n",
    "    words = []\n",
    "    for word in line: # line - iterable, for example list of tokens \n",
    "        try:\n",
    "            w2v_idx = indicies_wv[word]\n",
    "        except KeyError: # if you do not have a vector for this word in your model, continue \n",
    "            continue\n",
    "        words.append(vectors_wv[w2v_idx])\n",
    "        if words: \n",
    "            words = np.asarray(words)\n",
    "            min_vec = words.min(axis=0)\n",
    "            max_vec = words.max(axis=0)\n",
    "            return np.concatenate((min_vec, max_vec))\n",
    "        if not words:\n",
    "            return None "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "id": "4a7a9374",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = [vectorize(line_) for line_ in df['desc']]\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71ebacf2",
   "metadata": {},
   "source": [
    "## KMeans clustering using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "0b0cbf03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cluster, metrics\n",
    "\n",
    "NUM_CLUSTERS=5\n",
    "\n",
    "kmeans = cluster.KMeans(n_clusters=NUM_CLUSTERS)\n",
    "kmeans.fit(vectors_wv)\n",
    " \n",
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "0b00986f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-8.0469064e-04,  2.5542439e-03,  6.4649968e-04,  2.8309756e-04,\n",
       "       -7.9932419e-05, -3.7139379e-03,  6.5906555e-04,  4.1457983e-03,\n",
       "       -1.2015824e-03, -1.2586818e-03, -1.8448869e-03], dtype=float32)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "centroids[0][0:11]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "60e748b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "not here\n"
     ]
    }
   ],
   "source": [
    "0.16475916 in indicies_wv['man']\n",
    "\n",
    "def get_key(val):\n",
    "    for key_ in indicies_wv.keys():\n",
    "        if val in indicies_wv[key_]:\n",
    "            return print(key_)\n",
    "    return print('not here')\n",
    "\n",
    "get_it(-8.0469064e-04)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87fad048",
   "metadata": {},
   "source": [
    "## KMeans clustering using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "id": "6d6c645c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.cluster import KMeansClusterer, euclidean_distance\n",
    "NUM_CLUSTERS=5\n",
    "kclusterer = KMeansClusterer(NUM_CLUSTERS, distance=euclidean_distance, repeats=25)\n",
    "assigned_clusters = kclusterer.cluster(vectors_wv, assign_clusters=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "155b169a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4}"
      ]
     },
     "execution_count": 329,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(assigned_clusters) #test to confirm clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80eb77e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "def1c6c1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "id": "3a4af701",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df.title = df.title.apply(lambda x: [ps.stem(word) for word in x if word not in stopwords])\n",
      "df.desc = df.desc.apply(lambda x: [ps.stem(word) for word in x if word not in stopwords])\n",
      "Usage: ipykernel_launcher.py [options]\n",
      "\n",
      "Options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
      "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
      "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
      "  --use-hashing         Use a hashing feature vectorizer\n",
      "  --n-features=N_FEATURES\n",
      "                        Maximum number of features (dimensions) to extract\n",
      "                        from text.\n",
      "  --verbose             Print progress reports inside k-means algorithm.\n",
      "2891 documents\n",
      "\n",
      "Extracting features from the training dataset using a sparse vectorizer\n",
      "done in 0.381184s\n",
      "n_samples: 2891, n_features: 3419\n",
      "\n",
      "Clustering sparse data with MiniBatchKMeans(n_clusters=6, random_state=42, verbose=False)\n",
      "done in 0.393s\n",
      "\n",
      "Homogeneity: 0.134\n",
      "Completeness: 1.000\n",
      "V-measure: 0.236\n",
      "Adjusted Rand-Index: 0.000\n",
      "Silhouette Coefficient: 0.005\n",
      "\n",
      "Top terms per cluster:\n",
      "Cluster 0: want guide auburn miami colina jorge chief participating protest thousands\n",
      "Cluster 1: chief best carmen city council mayor durkan jenny spd diaz\n",
      "Cluster 2: city council budget defund million durkan cuts mayor cut voted\n",
      "Cluster 3: county king sheriff office pierce black jail lives said booked\n",
      "Cluster 4: said officers people man suspect spd officer arrested shooting year\n",
      "Cluster 5: spd officers according officer year man arrested precinct old people\n",
      "Cluster 6:"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 6 is out of bounds for axis 0 with size 6",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-428-fb0808aa84cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrue_k\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cluster %d:\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mind\u001b[0m \u001b[0;32min\u001b[0m \u001b[0morder_centroids\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mterms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mind\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: index 6 is out of bounds for axis 0 with size 6"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "import logging\n",
    "from optparse import OptionParser\n",
    "import sys\n",
    "from time import time\n",
    "\n",
    "\n",
    "\n",
    "# Display progress logs on stdout\n",
    "logging.basicConfig(level=logging.INFO,\n",
    "                    format='%(asctime)s %(levelname)s %(message)s')\n",
    "\n",
    "# parse commandline arguments\n",
    "op = OptionParser()\n",
    "op.add_option(\"--lsa\",\n",
    "              dest=\"n_components\", type=\"int\",\n",
    "              help=\"Preprocess documents with latent semantic analysis.\")\n",
    "op.add_option(\"--no-minibatch\",\n",
    "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
    "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
    "op.add_option(\"--no-idf\",\n",
    "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
    "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
    "op.add_option(\"--use-hashing\",\n",
    "              action=\"store_true\", default=False,\n",
    "              help=\"Use a hashing feature vectorizer\")\n",
    "op.add_option(\"--n-features\", type=int, default=10000,\n",
    "              help=\"Maximum number of features (dimensions)\"\n",
    "                   \" to extract from text.\")\n",
    "op.add_option(\"--verbose\",\n",
    "              action=\"store_true\", dest=\"verbose\", default=False,\n",
    "              help=\"Print progress reports inside k-means algorithm.\")\n",
    "\n",
    "print(__doc__)\n",
    "op.print_help()\n",
    "\n",
    "\n",
    "def is_interactive():\n",
    "    return not hasattr(sys.modules['__main__'], '__file__')\n",
    "\n",
    "\n",
    "# work-around for Jupyter notebook and IPython console\n",
    "argv = [] if is_interactive() else sys.argv[1:]\n",
    "(opts, args) = op.parse_args(argv)\n",
    "if len(args) > 0:\n",
    "    op.error(\"this script takes no arguments.\")\n",
    "    sys.exit(1)\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Load some categories from the training set\n",
    "\n",
    "# Uncomment the following to do the analysis on all the categories\n",
    "# categories = None\n",
    "\n",
    "\n",
    "\n",
    "dataset = test_df\n",
    "\n",
    "print(\"%d documents\" % len(dataset.desc))\n",
    "\n",
    "print()\n",
    "\n",
    "labels = dataset.desc\n",
    "true_k = np.unique(labels).shape[0]\n",
    "\n",
    "print(\"Extracting features from the training dataset \"\n",
    "      \"using a sparse vectorizer\")\n",
    "t0 = time()\n",
    "if opts.use_hashing:\n",
    "    if opts.use_idf:\n",
    "        # Perform an IDF normalization on the output of HashingVectorizer\n",
    "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
    "                                   stop_words='english', alternate_sign=False,\n",
    "                                   norm=None)\n",
    "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
    "    else:\n",
    "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
    "                                       stop_words='english',\n",
    "                                       alternate_sign=False, norm='l2')\n",
    "else:\n",
    "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
    "                                 min_df=2, stop_words='english',\n",
    "                                 use_idf=opts.use_idf)\n",
    "X = vectorizer.fit_transform(dataset.desc)\n",
    "\n",
    "print(\"done in %fs\" % (time() - t0))\n",
    "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
    "print()\n",
    "\n",
    "if opts.n_components:\n",
    "    print(\"Performing dimensionality reduction using LSA\")\n",
    "    t0 = time()\n",
    "    # Vectorizer results are normalized, which makes KMeans behave as\n",
    "    # spherical k-means for better results. Since LSA/SVD results are\n",
    "    # not normalized, we have to redo the normalization.\n",
    "    svd = TruncatedSVD(opts.n_components)\n",
    "    normalizer = Normalizer(copy=False)\n",
    "    lsa = make_pipeline(svd, normalizer)\n",
    "\n",
    "    X = lsa.fit_transform(X)\n",
    "\n",
    "    print(\"done in %fs\" % (time() - t0))\n",
    "\n",
    "    explained_variance = svd.explained_variance_ratio_.sum()\n",
    "    print(\"Explained variance of the SVD step: {}%\".format(\n",
    "        int(explained_variance * 100)))\n",
    "\n",
    "    print()\n",
    "\n",
    "\n",
    "# #############################################################################\n",
    "# Do the actual clustering\n",
    "\n",
    "if opts.minibatch:\n",
    "    km = MiniBatchKMeans(n_clusters=6, init='k-means++',\n",
    "                          verbose=opts.verbose, random_state=42)\n",
    "else:\n",
    "    km = KMeans(n_clusters=6, init='k-means++', max_iter=200, \n",
    "                verbose=opts.verbose, random_state=42)\n",
    "\n",
    "print(\"Clustering sparse data with %s\" % km)\n",
    "t0 = time()\n",
    "km.fit(X)\n",
    "print(\"done in %0.3fs\" % (time() - t0))\n",
    "print()\n",
    "\n",
    "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
    "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
    "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
    "print(\"Adjusted Rand-Index: %.3f\"\n",
    "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
    "print(\"Silhouette Coefficient: %0.3f\"\n",
    "      % metrics.silhouette_score(X, km.labels_))\n",
    "\n",
    "print()\n",
    "\n",
    "\n",
    "if not opts.use_hashing:\n",
    "    print(\"Top terms per cluster:\")\n",
    "\n",
    "    if opts.n_components:\n",
    "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
    "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "    else:\n",
    "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
    "\n",
    "    terms = vectorizer.get_feature_names()\n",
    "    for i in range(true_k):\n",
    "        print(\"Cluster %d:\" % i, end='')\n",
    "        for ind in order_centroids[i, :10]:\n",
    "            print(' %s' % terms[ind], end='')\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aba52eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
